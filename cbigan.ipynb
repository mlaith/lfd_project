{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290297b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of classes (for conditioning): 2\n",
      "CBIGAN training data shape: (96822, 190)\n",
      "CBIGAN testing data shape: (49971, 190)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "# We can keep most hyperparameters the same for a fair comparison\n",
    "LATENT_DIM = 128\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "W_ADV = 1    # Weight for Adversarial Loss\n",
    "W_CON = 50   # Weight for Contextual (Reconstruction) Loss\n",
    "W_ENC = 1    # Weight for Encoder Loss\n",
    "\n",
    "# --- Device Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load the Datasets for CBIGAN ---\n",
    "\n",
    "X_train = pd.read_parquet('processed_data/cbigan_X_train.parquet')\n",
    "y_train = pd.read_parquet('processed_data/cbigan_y_train.parquet')['label']\n",
    "X_test = pd.read_parquet('processed_data/cbigan_X_test.parquet')\n",
    "y_test = pd.read_parquet('processed_data/cbigan_y_test.parquet')['label']\n",
    "\n",
    "# --- One-Hot Encode the Labels ---\n",
    "# The condition 'y' needs to be in a numerical format for the model\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_one_hot = one_hot_encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_one_hot = one_hot_encoder.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "NUM_CLASSES = y_train_one_hot.shape[1]\n",
    "print(f\"Number of classes (for conditioning): {NUM_CLASSES}\")\n",
    "\n",
    "\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "\n",
    "# --- PyTorch Dataset and DataLoader for CBIGAN ---\n",
    "class ConditionalDataset(Dataset):\n",
    "    def __init__(self, features, labels_one_hot):\n",
    "        self.features = torch.tensor(features.values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels_one_hot, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_cbigan = ConditionalDataset(X_train, y_train_one_hot)\n",
    "test_dataset_cbigan = ConditionalDataset(X_test, y_test_one_hot)\n",
    "\n",
    "train_loader_cbigan = DataLoader(train_dataset_cbigan, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_cbigan = DataLoader(test_dataset_cbigan, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "print(f\"CBIGAN training data shape: {X_train.shape}\")\n",
    "print(f\"CBIGAN testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a95cbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generator Architecture ---\n",
      "Generator(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=190, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (latent_layer): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Linear(in_features=512, out_features=190, bias=True)\n",
      "    (6): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "--- Discriminator Architecture ---\n",
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=190, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (feature_layer): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (final_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.latent_layer = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, input_dim),\n",
    "            nn.Sigmoid()  # To output values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        z = self.latent_layer(encoded)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction, z\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.feature_layer = nn.Linear(256, 128)\n",
    "        self.final_layer = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "        feature_vector = self.feature_layer(features)\n",
    "        prediction = torch.sigmoid(self.final_layer(feature_vector))\n",
    "        return prediction.squeeze(), feature_vector\n",
    "\n",
    "# --- Instantiate the models and move to device ---\n",
    "generator = Generator(INPUT_DIM, LATENT_DIM).to(device)\n",
    "discriminator = Discriminator(INPUT_DIM).to(device)\n",
    "\n",
    "print(\"--- Generator Architecture ---\")\n",
    "print(generator)\n",
    "print(\"\\n--- Discriminator Architecture ---\")\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Re-balanced Training\n",
    "D_LEARNING_RATE = 1e-5   # Make the Discriminator learn slowly\n",
    "GE_LEARNING_RATE = 2e-4  # Allow the Generator/Encoder to learn faster\n",
    "\n",
    "encoder = Encoder(input_dim, latent_dim, NUM_CLASSES).to(device)\n",
    "generator = Generator(latent_dim, input_dim, NUM_CLASSES).to(device)\n",
    "discriminator = Discriminator(input_dim, latent_dim, NUM_CLASSES).to(device)\n",
    "\n",
    "# --- Loss Function for the Adversarial Game ---\n",
    "adversarial_loss = nn.BCELoss().to(device)\n",
    "\n",
    "# Optimizers with different learning rates\n",
    "# One for the Discriminator with a low LR\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=D_LEARNING_RATE, betas=(0.5, 0.999))\n",
    "\n",
    "# A second one for the Generator and Encoder team with a higher LR\n",
    "optimizer_ge = optim.Adam(\n",
    "    list(generator.parameters()) + list(encoder.parameters()),\n",
    "    lr=GE_LEARNING_RATE,\n",
    "    betas=(0.5, 0.999)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Corrected Optimizers and Loss function for CBIGAN have been set up.\")\n",
    "print(f\"Discriminator LR: {D_LEARNING_RATE}, Generator/Encoder LR: {GE_LEARNING_RATE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
