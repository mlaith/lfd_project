{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd572b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Shape of normal training data for CBIGAN: (56000, 186)\n",
      "Shape of full test data for evaluation: (82332, 186)\n",
      "Shape of test labels for evaluation: (82332,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# --- 0. Device Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. Data Loading and Preprocessing ---\n",
    "# Reload the datasets\n",
    "UNSW_training = pd.read_parquet(\"UNSW-NB15/UNSW_NB15_training-set.parquet\")\n",
    "UNSW_testing = pd.read_parquet(\"UNSW-NB15/UNSW_NB15_testing-set.parquet\")\n",
    "\n",
    "\n",
    "# Separate labels and attack_cat from features\n",
    "labels_train = UNSW_training[['label', 'attack_cat']]\n",
    "labels_test = UNSW_testing[['label', 'attack_cat']]\n",
    "\n",
    "# Drop 'label' and 'attack_cat' from the feature sets\n",
    "UNSW_training_features = UNSW_training.drop(columns=['label', 'attack_cat'])\n",
    "UNSW_testing_features = UNSW_testing.drop(columns=['label', 'attack_cat'])\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = UNSW_training_features.select_dtypes(include=['category', 'object']).columns\n",
    "numerical_cols = UNSW_training_features.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Preprocessing pipelines for numerical and categorical features\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Create a preprocessor using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_processed_array = preprocessor.fit_transform(UNSW_training_features)\n",
    "# Transform the testing data\n",
    "X_test_processed_array = preprocessor.transform(UNSW_testing_features)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "numerical_feature_names = list(numerical_cols)\n",
    "categorical_feature_names_ohe = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "all_processed_feature_names = numerical_feature_names + list(categorical_feature_names_ohe)\n",
    "\n",
    "# Convert processed arrays back to DataFrame (optional, but good for inspection)\n",
    "X_train_processed_df = pd.DataFrame(X_train_processed_array, columns=all_processed_feature_names)\n",
    "X_test_processed_df = pd.DataFrame(X_test_processed_array, columns=all_processed_feature_names)\n",
    "\n",
    "# Add back the labels and attack_cat\n",
    "X_train_processed_df['label'] = labels_train['label'].values\n",
    "X_train_processed_df['attack_cat'] = labels_train['attack_cat'].values\n",
    "X_test_processed_df['label'] = labels_test['label'].values\n",
    "X_test_processed_df['attack_cat'] = labels_test['attack_cat'].values\n",
    "\n",
    "# Filter for normal data for training the GANs\n",
    "normal_train_data = X_train_processed_df[X_train_processed_df['label'] == 0].drop(columns=['label', 'attack_cat'])\n",
    "X_train_normal_np = normal_train_data.values.astype(np.float32)\n",
    "\n",
    "# Prepare full test data and labels for evaluation\n",
    "X_test_full_np = X_test_processed_df.drop(columns=['label', 'attack_cat']).values.astype(np.float32)\n",
    "y_test_full = X_test_processed_df['label'].values\n",
    "\n",
    "print(f\"Shape of normal training data for CBIGAN: {X_train_normal_np.shape}\")\n",
    "print(f\"Shape of full test data for evaluation: {X_test_full_np.shape}\")\n",
    "print(f\"Shape of test labels for evaluation: {y_test_full.shape}\")\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_normal_tensor = torch.from_numpy(X_train_normal_np).to(device)\n",
    "X_test_full_tensor = torch.from_numpy(X_test_full_np).to(device)\n",
    "y_test_full_tensor = torch.from_numpy(y_test_full).to(device)\n",
    "\n",
    "# Create PyTorch DataLoader\n",
    "BATCH_SIZE = 256\n",
    "train_dataset = TensorDataset(X_train_normal_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94620d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. CBIGAN Model Architecture in PyTorch ---\n",
    "\n",
    "def init_weights(m):\n",
    "    \"\"\"Custom weight initialization for linear layers\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder E: Maps data x to latent representation z_x\"\"\"\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, latent_dim),\n",
    "            nn.Tanh() # Latent vectors typically in [-1, 1]\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder G: Maps latent representation z to data x_z\"\"\"\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, output_dim) # Output linear as data is standardized\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.main(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator D: Takes (data, latent) pairs and outputs a score.\n",
    "    For CBIGAN (and BiGAN), it distinguishes (x, E(x)) from (G(z), z).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Discriminator takes concatenated (data, latent)\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim + latent_dim, 512), # Input is concatenated x and z\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1) # Output is raw score for WGAN, no sigmoid\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        # Concatenate data and latent vector\n",
    "        x_z = torch.cat([x, z], 1)\n",
    "        return self.main(x_z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5cccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting CBIGAN training...\n",
      "Epoch 1/100, Batch 0/218: D Loss: -0.0055, GP: 0.8854, G Adv Loss: -0.0058, Consistency Loss: 0.7934\n",
      "Epoch 1/100, Batch 50/218: D Loss: -53.2252, GP: 1.3851, G Adv Loss: -55.1455, Consistency Loss: 0.8020\n",
      "Epoch 1/100, Batch 100/218: D Loss: -2780.1335, GP: 129.3901, G Adv Loss: -2741.6548, Consistency Loss: 0.8424\n",
      "Epoch 1/100, Batch 150/218: D Loss: -37759.2539, GP: 1790.3772, G Adv Loss: -37314.5625, Consistency Loss: 1.1699\n",
      "Epoch 1/100, Batch 200/218: D Loss: -232621.4375, GP: 11460.8828, G Adv Loss: -229934.8281, Consistency Loss: 1.1562\n",
      "Epoch 1 completed. D Loss: -389645.5938, G Loss: -380980.5312\n",
      "Epoch 2/100, Batch 0/218: D Loss: -394171.4688, GP: 20041.6289, G Adv Loss: -399034.9062, Consistency Loss: 1.1585\n",
      "Epoch 2/100, Batch 50/218: D Loss: -1411978.1250, GP: 69466.9062, G Adv Loss: -1438163.7500, Consistency Loss: 1.1681\n",
      "Epoch 2/100, Batch 100/218: D Loss: -4073746.7500, GP: 200902.8438, G Adv Loss: -4059223.2500, Consistency Loss: 1.1723\n",
      "Epoch 2/100, Batch 150/218: D Loss: -10017180.0000, GP: 498841.5938, G Adv Loss: -10003806.0000, Consistency Loss: 1.1639\n",
      "Epoch 2/100, Batch 200/218: D Loss: -20908100.0000, GP: 1055974.7500, G Adv Loss: -20662428.0000, Consistency Loss: 1.1674\n",
      "Epoch 2 completed. D Loss: -27684474.0000, G Loss: -27139626.0000\n",
      "Epoch 3/100, Batch 0/218: D Loss: -27329098.0000, GP: 1361549.2500, G Adv Loss: -26618528.0000, Consistency Loss: 1.1570\n",
      "Epoch 3/100, Batch 50/218: D Loss: -50844844.0000, GP: 2501836.7500, G Adv Loss: -50708716.0000, Consistency Loss: 1.1760\n",
      "Epoch 3/100, Batch 100/218: D Loss: -90752992.0000, GP: 4569846.0000, G Adv Loss: -90330248.0000, Consistency Loss: 1.1597\n",
      "Epoch 3/100, Batch 150/218: D Loss: -151073136.0000, GP: 7665824.0000, G Adv Loss: -147436496.0000, Consistency Loss: 1.1680\n",
      "Epoch 3/100, Batch 200/218: D Loss: -238447120.0000, GP: 11778044.0000, G Adv Loss: -237577904.0000, Consistency Loss: 1.1609\n",
      "Epoch 3 completed. D Loss: -281075840.0000, G Loss: -286678688.0000\n",
      "Epoch 4/100, Batch 0/218: D Loss: -284255104.0000, GP: 13905904.0000, G Adv Loss: -278239872.0000, Consistency Loss: 1.1658\n",
      "Epoch 4/100, Batch 50/218: D Loss: -431809376.0000, GP: 21536256.0000, G Adv Loss: -439733088.0000, Consistency Loss: 1.1605\n",
      "Epoch 4/100, Batch 100/218: D Loss: -637366912.0000, GP: 31508050.0000, G Adv Loss: -632196352.0000, Consistency Loss: 1.1686\n",
      "Epoch 4/100, Batch 150/218: D Loss: -921442624.0000, GP: 44694608.0000, G Adv Loss: -904991360.0000, Consistency Loss: 1.1676\n",
      "Epoch 4/100, Batch 200/218: D Loss: -1292289664.0000, GP: 63469016.0000, G Adv Loss: -1270178688.0000, Consistency Loss: 1.1647\n",
      "Epoch 4 completed. D Loss: -1418022016.0000, G Loss: -1434681600.0000\n",
      "Epoch 5/100, Batch 0/218: D Loss: -1465318144.0000, GP: 70583840.0000, G Adv Loss: -1461412864.0000, Consistency Loss: 1.1651\n",
      "Epoch 5/100, Batch 50/218: D Loss: -1934410496.0000, GP: 96119472.0000, G Adv Loss: -1908468096.0000, Consistency Loss: 1.1727\n",
      "Epoch 5/100, Batch 100/218: D Loss: -2619127552.0000, GP: 127726560.0000, G Adv Loss: -2578841344.0000, Consistency Loss: 1.1671\n",
      "Epoch 5/100, Batch 150/218: D Loss: -3446110464.0000, GP: 169390336.0000, G Adv Loss: -3427103488.0000, Consistency Loss: 1.1623\n",
      "Epoch 5/100, Batch 200/218: D Loss: -4447513600.0000, GP: 220253584.0000, G Adv Loss: -4404232192.0000, Consistency Loss: 1.1690\n",
      "Epoch 5 completed. D Loss: -4937002496.0000, G Loss: -4884557824.0000\n",
      "Epoch 6/100, Batch 0/218: D Loss: -4861077504.0000, GP: 246138304.0000, G Adv Loss: -4859667456.0000, Consistency Loss: 1.1645\n",
      "Epoch 6/100, Batch 50/218: D Loss: -6200727552.0000, GP: 308046912.0000, G Adv Loss: -6240394240.0000, Consistency Loss: 1.1672\n",
      "Epoch 6/100, Batch 100/218: D Loss: -7686362624.0000, GP: 390921600.0000, G Adv Loss: -7855542272.0000, Consistency Loss: 1.1643\n",
      "Epoch 6/100, Batch 150/218: D Loss: -9696640000.0000, GP: 478259936.0000, G Adv Loss: -9699898368.0000, Consistency Loss: 1.1643\n",
      "Epoch 6/100, Batch 200/218: D Loss: -12301496320.0000, GP: 594523136.0000, G Adv Loss: -11922728960.0000, Consistency Loss: 1.1578\n",
      "Epoch 6 completed. D Loss: -12983688192.0000, G Loss: -12872553472.0000\n",
      "Epoch 7/100, Batch 0/218: D Loss: -12790413312.0000, GP: 639846464.0000, G Adv Loss: -12818742272.0000, Consistency Loss: 1.1616\n",
      "Epoch 7/100, Batch 50/218: D Loss: -16109660160.0000, GP: 782430272.0000, G Adv Loss: -15504174080.0000, Consistency Loss: 1.1671\n",
      "Epoch 7/100, Batch 100/218: D Loss: -18924339200.0000, GP: 951118208.0000, G Adv Loss: -19001159680.0000, Consistency Loss: 1.1621\n",
      "Epoch 7/100, Batch 150/218: D Loss: -22377476096.0000, GP: 1121998080.0000, G Adv Loss: -22685712384.0000, Consistency Loss: 1.1656\n",
      "Epoch 7/100, Batch 200/218: D Loss: -27283167232.0000, GP: 1338728448.0000, G Adv Loss: -27705286656.0000, Consistency Loss: 1.1697\n",
      "Epoch 7 completed. D Loss: -29178757120.0000, G Loss: -29034065920.0000\n",
      "Epoch 8/100, Batch 0/218: D Loss: -29027516416.0000, GP: 1431059200.0000, G Adv Loss: -28887173120.0000, Consistency Loss: 1.1696\n",
      "Epoch 8/100, Batch 50/218: D Loss: -34107254784.0000, GP: 1693782016.0000, G Adv Loss: -33899620352.0000, Consistency Loss: 1.1647\n",
      "Epoch 8/100, Batch 100/218: D Loss: -39688351744.0000, GP: 1992066944.0000, G Adv Loss: -39391465472.0000, Consistency Loss: 1.1692\n",
      "Epoch 8/100, Batch 150/218: D Loss: -46054236160.0000, GP: 2347739136.0000, G Adv Loss: -46132551680.0000, Consistency Loss: 1.1688\n",
      "Epoch 8/100, Batch 200/218: D Loss: -54058270720.0000, GP: 2709089792.0000, G Adv Loss: -53667282944.0000, Consistency Loss: 1.1682\n",
      "Epoch 8 completed. D Loss: -57393930240.0000, G Loss: -57044463616.0000\n",
      "Epoch 9/100, Batch 0/218: D Loss: -57623871488.0000, GP: 2847370752.0000, G Adv Loss: -56874921984.0000, Consistency Loss: 1.1648\n",
      "Epoch 9/100, Batch 50/218: D Loss: -66746368000.0000, GP: 3263093760.0000, G Adv Loss: -66091401216.0000, Consistency Loss: 1.1646\n",
      "Epoch 9/100, Batch 100/218: D Loss: -76547072000.0000, GP: 3765284864.0000, G Adv Loss: -76328493056.0000, Consistency Loss: 1.1695\n",
      "Epoch 9/100, Batch 150/218: D Loss: -87402668032.0000, GP: 4326530560.0000, G Adv Loss: -86551961600.0000, Consistency Loss: 1.1663\n",
      "Epoch 9/100, Batch 200/218: D Loss: -100802281472.0000, GP: 4939718656.0000, G Adv Loss: -100373315584.0000, Consistency Loss: 1.1764\n",
      "Epoch 9 completed. D Loss: -104339226624.0000, G Loss: -103015522304.0000\n",
      "Epoch 10/100, Batch 0/218: D Loss: -104797839360.0000, GP: 5135543296.0000, G Adv Loss: -104309465088.0000, Consistency Loss: 1.1695\n",
      "Epoch 10/100, Batch 50/218: D Loss: -117453135872.0000, GP: 5864187904.0000, G Adv Loss: -119175774208.0000, Consistency Loss: 1.1589\n",
      "Epoch 10/100, Batch 100/218: D Loss: -132647346176.0000, GP: 6590154752.0000, G Adv Loss: -135355105280.0000, Consistency Loss: 1.1655\n",
      "Epoch 10/100, Batch 150/218: D Loss: -148028637184.0000, GP: 7475210240.0000, G Adv Loss: -152284299264.0000, Consistency Loss: 1.1581\n",
      "Epoch 10/100, Batch 200/218: D Loss: -174970847232.0000, GP: 8377980416.0000, G Adv Loss: -166588022784.0000, Consistency Loss: 1.1565\n",
      "Epoch 10 completed. D Loss: -174366277632.0000, G Loss: -178640699392.0000\n",
      "Epoch 11/100, Batch 0/218: D Loss: -177043161088.0000, GP: 8790328320.0000, G Adv Loss: -176240984064.0000, Consistency Loss: 1.1648\n",
      "Epoch 11/100, Batch 50/218: D Loss: -196780064768.0000, GP: 9712371712.0000, G Adv Loss: -194613526528.0000, Consistency Loss: 1.1657\n",
      "Epoch 11/100, Batch 100/218: D Loss: -218361921536.0000, GP: 10782260224.0000, G Adv Loss: -219906326528.0000, Consistency Loss: 1.1655\n",
      "Epoch 11/100, Batch 150/218: D Loss: -246884745216.0000, GP: 12213297152.0000, G Adv Loss: -243827720192.0000, Consistency Loss: 1.1691\n",
      "Epoch 11/100, Batch 200/218: D Loss: -272456826880.0000, GP: 13436163072.0000, G Adv Loss: -268548161536.0000, Consistency Loss: 1.1719\n",
      "Epoch 11 completed. D Loss: -284375941120.0000, G Loss: -283883405312.0000\n",
      "Epoch 12/100, Batch 0/218: D Loss: -279480958976.0000, GP: 13923948544.0000, G Adv Loss: -283081768960.0000, Consistency Loss: 1.1677\n",
      "Epoch 12/100, Batch 50/218: D Loss: -313594576896.0000, GP: 15262289920.0000, G Adv Loss: -310582870016.0000, Consistency Loss: 1.1713\n",
      "Epoch 12/100, Batch 100/218: D Loss: -344503156736.0000, GP: 17036105728.0000, G Adv Loss: -346795147264.0000, Consistency Loss: 1.1692\n",
      "Epoch 12/100, Batch 150/218: D Loss: -375130062848.0000, GP: 18916702208.0000, G Adv Loss: -380851093504.0000, Consistency Loss: 1.1650\n",
      "Epoch 12/100, Batch 200/218: D Loss: -404582989824.0000, GP: 20349691904.0000, G Adv Loss: -416400244736.0000, Consistency Loss: 1.1748\n",
      "Epoch 12 completed. D Loss: -435385729024.0000, G Loss: -431036006400.0000\n",
      "Epoch 13/100, Batch 0/218: D Loss: -417331478528.0000, GP: 21446897664.0000, G Adv Loss: -431313944576.0000, Consistency Loss: 1.1645\n",
      "Epoch 13/100, Batch 50/218: D Loss: -469410742272.0000, GP: 23007524864.0000, G Adv Loss: -465835622400.0000, Consistency Loss: 1.1680\n",
      "Epoch 13/100, Batch 100/218: D Loss: -511710396416.0000, GP: 25651531776.0000, G Adv Loss: -510808162304.0000, Consistency Loss: 1.1630\n",
      "Epoch 13/100, Batch 150/218: D Loss: -552538800128.0000, GP: 27495368704.0000, G Adv Loss: -569778372608.0000, Consistency Loss: 1.1597\n",
      "Epoch 13/100, Batch 200/218: D Loss: -610458664960.0000, GP: 30394855424.0000, G Adv Loss: -595701530624.0000, Consistency Loss: 1.1773\n",
      "Epoch 13 completed. D Loss: -620708167680.0000, G Loss: -642572419072.0000\n",
      "Epoch 14/100, Batch 0/218: D Loss: -638216306688.0000, GP: 31328043008.0000, G Adv Loss: -633219842048.0000, Consistency Loss: 1.1669\n",
      "Epoch 14/100, Batch 50/218: D Loss: -685440499712.0000, GP: 33904283648.0000, G Adv Loss: -679731134464.0000, Consistency Loss: 1.1621\n",
      "Epoch 14/100, Batch 100/218: D Loss: -733814456320.0000, GP: 36870311936.0000, G Adv Loss: -744719384576.0000, Consistency Loss: 1.1647\n",
      "Epoch 14/100, Batch 150/218: D Loss: -803450060800.0000, GP: 39747362816.0000, G Adv Loss: -799827427328.0000, Consistency Loss: 1.1660\n",
      "Epoch 14/100, Batch 200/218: D Loss: -868216864768.0000, GP: 42983415808.0000, G Adv Loss: -857771081728.0000, Consistency Loss: 1.1734\n",
      "Epoch 14 completed. D Loss: -882768216064.0000, G Loss: -878190985216.0000\n",
      "Epoch 15/100, Batch 0/218: D Loss: -896114950144.0000, GP: 44305276928.0000, G Adv Loss: -900939710464.0000, Consistency Loss: 1.1762\n",
      "Epoch 15/100, Batch 50/218: D Loss: -979054755840.0000, GP: 47857704960.0000, G Adv Loss: -954055131136.0000, Consistency Loss: 1.1663\n",
      "Epoch 15/100, Batch 100/218: D Loss: -1046362259456.0000, GP: 51695828992.0000, G Adv Loss: -1029773066240.0000, Consistency Loss: 1.1586\n",
      "Epoch 15/100, Batch 150/218: D Loss: -1129191833600.0000, GP: 55739097088.0000, G Adv Loss: -1115637153792.0000, Consistency Loss: 1.1633\n",
      "Epoch 15/100, Batch 200/218: D Loss: -1199383379968.0000, GP: 59378204672.0000, G Adv Loss: -1222769115136.0000, Consistency Loss: 1.1674\n",
      "Epoch 15 completed. D Loss: -1213769318400.0000, G Loss: -1224508440576.0000\n",
      "Epoch 16/100, Batch 0/218: D Loss: -1234091376640.0000, GP: 61379608576.0000, G Adv Loss: -1248575619072.0000, Consistency Loss: 1.1680\n",
      "Epoch 16/100, Batch 50/218: D Loss: -1332404682752.0000, GP: 65213513728.0000, G Adv Loss: -1339790721024.0000, Consistency Loss: 1.1566\n",
      "Epoch 16/100, Batch 100/218: D Loss: -1447737688064.0000, GP: 70398377984.0000, G Adv Loss: -1399175118848.0000, Consistency Loss: 1.1632\n",
      "Epoch 16/100, Batch 150/218: D Loss: -1527200874496.0000, GP: 76597149696.0000, G Adv Loss: -1528632442880.0000, Consistency Loss: 1.1631\n",
      "Epoch 16/100, Batch 200/218: D Loss: -1609019949056.0000, GP: 80220012544.0000, G Adv Loss: -1627295318016.0000, Consistency Loss: 1.1686\n",
      "Epoch 16 completed. D Loss: -1641917710336.0000, G Loss: -1632961560576.0000\n",
      "Epoch 17/100, Batch 0/218: D Loss: -1685134508032.0000, GP: 82653102080.0000, G Adv Loss: -1619682394112.0000, Consistency Loss: 1.1678\n",
      "Epoch 17/100, Batch 50/218: D Loss: -1756677931008.0000, GP: 88598380544.0000, G Adv Loss: -1808504586240.0000, Consistency Loss: 1.1627\n",
      "Epoch 17/100, Batch 100/218: D Loss: -1906156634112.0000, GP: 94275813376.0000, G Adv Loss: -1906908069888.0000, Consistency Loss: 1.1672\n",
      "Epoch 17/100, Batch 150/218: D Loss: -2018732277760.0000, GP: 100751925248.0000, G Adv Loss: -2041250578432.0000, Consistency Loss: 1.1739\n",
      "Epoch 17/100, Batch 200/218: D Loss: -2164457865216.0000, GP: 106869309440.0000, G Adv Loss: -2131985432576.0000, Consistency Loss: 1.1634\n",
      "Epoch 17 completed. D Loss: -2183672889344.0000, G Loss: -2238781849600.0000\n",
      "Epoch 18/100, Batch 0/218: D Loss: -2180659937280.0000, GP: 108769148928.0000, G Adv Loss: -2213461622784.0000, Consistency Loss: 1.1706\n",
      "Epoch 18/100, Batch 50/218: D Loss: -2305517682688.0000, GP: 116923047936.0000, G Adv Loss: -2352091234304.0000, Consistency Loss: 1.1642\n",
      "Epoch 18/100, Batch 100/218: D Loss: -2542120468480.0000, GP: 123169128448.0000, G Adv Loss: -2492829532160.0000, Consistency Loss: 1.1635\n",
      "Epoch 18/100, Batch 150/218: D Loss: -2660878778368.0000, GP: 131744530432.0000, G Adv Loss: -2646774382592.0000, Consistency Loss: 1.1581\n",
      "Epoch 18/100, Batch 200/218: D Loss: -2812990717952.0000, GP: 140345589760.0000, G Adv Loss: -2818851209216.0000, Consistency Loss: 1.1776\n",
      "Epoch 18 completed. D Loss: -2810570080256.0000, G Loss: -2863900655616.0000\n",
      "Epoch 19/100, Batch 0/218: D Loss: -2926426193920.0000, GP: 142859894784.0000, G Adv Loss: -2820373741568.0000, Consistency Loss: 1.1556\n",
      "Epoch 19/100, Batch 50/218: D Loss: -3023825797120.0000, GP: 151311351808.0000, G Adv Loss: -3037461741568.0000, Consistency Loss: 1.1643\n",
      "Epoch 19/100, Batch 100/218: D Loss: -3199195676672.0000, GP: 160073695232.0000, G Adv Loss: -3157100331008.0000, Consistency Loss: 1.1649\n",
      "Epoch 19/100, Batch 150/218: D Loss: -3460715511808.0000, GP: 169718841344.0000, G Adv Loss: -3440908435456.0000, Consistency Loss: 1.1731\n",
      "Epoch 19/100, Batch 200/218: D Loss: -3606254977024.0000, GP: 181229977600.0000, G Adv Loss: -3631875358720.0000, Consistency Loss: 1.1641\n",
      "Epoch 19 completed. D Loss: -3767678795776.0000, G Loss: -3651132194816.0000\n",
      "Epoch 20/100, Batch 0/218: D Loss: -3683797958656.0000, GP: 182457597952.0000, G Adv Loss: -3674589888512.0000, Consistency Loss: 1.1665\n",
      "Epoch 20/100, Batch 50/218: D Loss: -3855025700864.0000, GP: 189475930112.0000, G Adv Loss: -3882872995840.0000, Consistency Loss: 1.1626\n",
      "Epoch 20/100, Batch 100/218: D Loss: -4075868651520.0000, GP: 200145731584.0000, G Adv Loss: -4107963727872.0000, Consistency Loss: 1.1671\n",
      "Epoch 20/100, Batch 150/218: D Loss: -4245982543872.0000, GP: 212305903616.0000, G Adv Loss: -4348672999424.0000, Consistency Loss: 1.1600\n",
      "Epoch 20/100, Batch 200/218: D Loss: -4485869993984.0000, GP: 225044594688.0000, G Adv Loss: -4485713756160.0000, Consistency Loss: 1.1611\n",
      "Epoch 20 completed. D Loss: -4484480106496.0000, G Loss: -4547783688192.0000\n",
      "Epoch 21/100, Batch 0/218: D Loss: -4680942878720.0000, GP: 228462297088.0000, G Adv Loss: -4740295426048.0000, Consistency Loss: 1.1619\n",
      "Epoch 21/100, Batch 50/218: D Loss: -4935983824896.0000, GP: 243511836672.0000, G Adv Loss: -4914144083968.0000, Consistency Loss: 1.1632\n",
      "Epoch 21/100, Batch 100/218: D Loss: -5195894358016.0000, GP: 257817116672.0000, G Adv Loss: -5057589280768.0000, Consistency Loss: 1.1673\n",
      "Epoch 21/100, Batch 150/218: D Loss: -5439491145728.0000, GP: 265037840384.0000, G Adv Loss: -5407684689920.0000, Consistency Loss: 1.1627\n",
      "Epoch 21/100, Batch 200/218: D Loss: -5679965274112.0000, GP: 282492043264.0000, G Adv Loss: -5730347253760.0000, Consistency Loss: 1.1729\n",
      "Epoch 21 completed. D Loss: -5709544554496.0000, G Loss: -5691851931648.0000\n",
      "Epoch 22/100, Batch 0/218: D Loss: -5762404843520.0000, GP: 285847388160.0000, G Adv Loss: -5672923037696.0000, Consistency Loss: 1.1608\n",
      "Epoch 22/100, Batch 50/218: D Loss: -6025370402816.0000, GP: 303132016640.0000, G Adv Loss: -6076077441024.0000, Consistency Loss: 1.1662\n",
      "Epoch 22/100, Batch 100/218: D Loss: -6366736941056.0000, GP: 314570113024.0000, G Adv Loss: -6348590809088.0000, Consistency Loss: 1.1729\n",
      "Epoch 22/100, Batch 150/218: D Loss: -6682689142784.0000, GP: 334610169856.0000, G Adv Loss: -6626338144256.0000, Consistency Loss: 1.1647\n",
      "Epoch 22/100, Batch 200/218: D Loss: -6942597578752.0000, GP: 350407950336.0000, G Adv Loss: -7031102636032.0000, Consistency Loss: 1.1717\n",
      "Epoch 22 completed. D Loss: -7100273000448.0000, G Loss: -7022659502080.0000\n",
      "Epoch 23/100, Batch 0/218: D Loss: -7023678717952.0000, GP: 350144757760.0000, G Adv Loss: -7032469979136.0000, Consistency Loss: 1.1728\n",
      "Epoch 23/100, Batch 50/218: D Loss: -7514433781760.0000, GP: 369576542208.0000, G Adv Loss: -7455538413568.0000, Consistency Loss: 1.1675\n",
      "Epoch 23/100, Batch 100/218: D Loss: -7832198971392.0000, GP: 390006636544.0000, G Adv Loss: -7750167298048.0000, Consistency Loss: 1.1602\n",
      "Epoch 23/100, Batch 150/218: D Loss: -8300150128640.0000, GP: 412067823616.0000, G Adv Loss: -8058971357184.0000, Consistency Loss: 1.1656\n",
      "Epoch 23/100, Batch 200/218: D Loss: -8644857430016.0000, GP: 423813644288.0000, G Adv Loss: -8352554811392.0000, Consistency Loss: 1.1672\n",
      "Epoch 23 completed. D Loss: -8770369355776.0000, G Loss: -8872736587776.0000\n",
      "Epoch 24/100, Batch 0/218: D Loss: -8732484829184.0000, GP: 431642148864.0000, G Adv Loss: -8580303421440.0000, Consistency Loss: 1.1654\n",
      "Epoch 24/100, Batch 50/218: D Loss: -9020265988096.0000, GP: 452266426368.0000, G Adv Loss: -8905869492224.0000, Consistency Loss: 1.1685\n",
      "Epoch 24/100, Batch 100/218: D Loss: -9533457956864.0000, GP: 467476611072.0000, G Adv Loss: -9401301729280.0000, Consistency Loss: 1.1709\n",
      "Epoch 24/100, Batch 150/218: D Loss: -9863054753792.0000, GP: 488395898880.0000, G Adv Loss: -10073604620288.0000, Consistency Loss: 1.1709\n",
      "Epoch 24/100, Batch 200/218: D Loss: -10493043408896.0000, GP: 520542846976.0000, G Adv Loss: -10579958824960.0000, Consistency Loss: 1.1708\n",
      "Epoch 24 completed. D Loss: -10632379236352.0000, G Loss: -10327888494592.0000\n",
      "Epoch 25/100, Batch 0/218: D Loss: -10552134860800.0000, GP: 520389525504.0000, G Adv Loss: -10317979451392.0000, Consistency Loss: 1.1647\n",
      "Epoch 25/100, Batch 50/218: D Loss: -10938186989568.0000, GP: 548840734720.0000, G Adv Loss: -10938728054784.0000, Consistency Loss: 1.1615\n",
      "Epoch 25/100, Batch 100/218: D Loss: -11416379588608.0000, GP: 571468283904.0000, G Adv Loss: -11374373634048.0000, Consistency Loss: 1.1686\n",
      "Epoch 25/100, Batch 150/218: D Loss: -12101718376448.0000, GP: 588461244416.0000, G Adv Loss: -12191707168768.0000, Consistency Loss: 1.1660\n",
      "Epoch 25/100, Batch 200/218: D Loss: -12587458625536.0000, GP: 613182996480.0000, G Adv Loss: -12391205044224.0000, Consistency Loss: 1.1579\n",
      "Epoch 25 completed. D Loss: -12516890509312.0000, G Loss: -12381018128384.0000\n",
      "Epoch 26/100, Batch 0/218: D Loss: -12794780975104.0000, GP: 631473242112.0000, G Adv Loss: -12587988156416.0000, Consistency Loss: 1.1599\n",
      "Epoch 26/100, Batch 50/218: D Loss: -13026581282816.0000, GP: 654141816832.0000, G Adv Loss: -12927543279616.0000, Consistency Loss: 1.1692\n",
      "Epoch 26/100, Batch 100/218: D Loss: -13624609341440.0000, GP: 678264373248.0000, G Adv Loss: -13855479562240.0000, Consistency Loss: 1.1720\n",
      "Epoch 26/100, Batch 150/218: D Loss: -14349158580224.0000, GP: 708614750208.0000, G Adv Loss: -14113456521216.0000, Consistency Loss: 1.1687\n",
      "Epoch 26/100, Batch 200/218: D Loss: -14956592365568.0000, GP: 738702786560.0000, G Adv Loss: -14967543693312.0000, Consistency Loss: 1.1583\n",
      "Epoch 26 completed. D Loss: -14869845770240.0000, G Loss: -15268336107520.0000\n",
      "Epoch 27/100, Batch 0/218: D Loss: -14833324916736.0000, GP: 753442816000.0000, G Adv Loss: -14936583438336.0000, Consistency Loss: 1.1659\n",
      "Epoch 27/100, Batch 50/218: D Loss: -15662723366912.0000, GP: 788354564096.0000, G Adv Loss: -15711256707072.0000, Consistency Loss: 1.1561\n",
      "Epoch 27/100, Batch 100/218: D Loss: -16256146079744.0000, GP: 801625735168.0000, G Adv Loss: -15788784222208.0000, Consistency Loss: 1.1676\n",
      "Epoch 27/100, Batch 150/218: D Loss: -16687883616256.0000, GP: 836723802112.0000, G Adv Loss: -16881522049024.0000, Consistency Loss: 1.1676\n",
      "Epoch 27/100, Batch 200/218: D Loss: -17668822269952.0000, GP: 869940592640.0000, G Adv Loss: -17423430320128.0000, Consistency Loss: 1.1637\n",
      "Epoch 27 completed. D Loss: -17681925275648.0000, G Loss: -17851752644608.0000\n",
      "Epoch 28/100, Batch 0/218: D Loss: -17761438793728.0000, GP: 884860780544.0000, G Adv Loss: -17803165827072.0000, Consistency Loss: 1.1643\n",
      "Epoch 28/100, Batch 50/218: D Loss: -18440463056896.0000, GP: 914898157568.0000, G Adv Loss: -18583377674240.0000, Consistency Loss: 1.1678\n",
      "Epoch 28/100, Batch 100/218: D Loss: -18944695992320.0000, GP: 941116882944.0000, G Adv Loss: -19064491606016.0000, Consistency Loss: 1.1659\n",
      "Epoch 28/100, Batch 150/218: D Loss: -19890408783872.0000, GP: 993979924480.0000, G Adv Loss: -19755400429568.0000, Consistency Loss: 1.1686\n",
      "Epoch 28/100, Batch 200/218: D Loss: -20697709543424.0000, GP: 1031765819392.0000, G Adv Loss: -20981389197312.0000, Consistency Loss: 1.1693\n",
      "Epoch 28 completed. D Loss: -21200455598080.0000, G Loss: -20847821586432.0000\n",
      "Epoch 29/100, Batch 0/218: D Loss: -20659681886208.0000, GP: 1034596777984.0000, G Adv Loss: -21045322973184.0000, Consistency Loss: 1.1665\n",
      "Epoch 29/100, Batch 50/218: D Loss: -21459145588736.0000, GP: 1066395107328.0000, G Adv Loss: -21894038290432.0000, Consistency Loss: 1.1615\n",
      "Epoch 29/100, Batch 100/218: D Loss: -22154605232128.0000, GP: 1097742548992.0000, G Adv Loss: -22742973808640.0000, Consistency Loss: 1.1721\n",
      "Epoch 29/100, Batch 150/218: D Loss: -23415209590784.0000, GP: 1149574840320.0000, G Adv Loss: -22828957040640.0000, Consistency Loss: 1.1638\n",
      "Epoch 29/100, Batch 200/218: D Loss: -23833815810048.0000, GP: 1202588483584.0000, G Adv Loss: -23626174693376.0000, Consistency Loss: 1.1663\n",
      "Epoch 29 completed. D Loss: -24254072487936.0000, G Loss: -24477347872768.0000\n",
      "Epoch 30/100, Batch 0/218: D Loss: -24149720301568.0000, GP: 1211799699456.0000, G Adv Loss: -24959581683712.0000, Consistency Loss: 1.1632\n",
      "Epoch 30/100, Batch 50/218: D Loss: -25262844542976.0000, GP: 1249124024320.0000, G Adv Loss: -25039835496448.0000, Consistency Loss: 1.1704\n",
      "Epoch 30/100, Batch 100/218: D Loss: -26541125795840.0000, GP: 1307407024128.0000, G Adv Loss: -25734043140096.0000, Consistency Loss: 1.1736\n",
      "Epoch 30/100, Batch 150/218: D Loss: -26756432003072.0000, GP: 1328460464128.0000, G Adv Loss: -26598598246400.0000, Consistency Loss: 1.1648\n",
      "Epoch 30/100, Batch 200/218: D Loss: -27417477382144.0000, GP: 1389350223872.0000, G Adv Loss: -27940460953600.0000, Consistency Loss: 1.1722\n",
      "Epoch 30 completed. D Loss: -28356028399616.0000, G Loss: -27660335972352.0000\n",
      "Epoch 31/100, Batch 0/218: D Loss: -27702862020608.0000, GP: 1394981732352.0000, G Adv Loss: -27632005545984.0000, Consistency Loss: 1.1585\n",
      "Epoch 31/100, Batch 50/218: D Loss: -29048902254592.0000, GP: 1429167407104.0000, G Adv Loss: -29818280214528.0000, Consistency Loss: 1.1646\n",
      "Epoch 31/100, Batch 100/218: D Loss: -29939231358976.0000, GP: 1493144174592.0000, G Adv Loss: -29701819072512.0000, Consistency Loss: 1.1659\n",
      "Epoch 31/100, Batch 150/218: D Loss: -31585378238464.0000, GP: 1558041067520.0000, G Adv Loss: -31727829385216.0000, Consistency Loss: 1.1611\n",
      "Epoch 31/100, Batch 200/218: D Loss: -31485478305792.0000, GP: 1608918499328.0000, G Adv Loss: -32212359577600.0000, Consistency Loss: 1.1721\n",
      "Epoch 31 completed. D Loss: -32466416959488.0000, G Loss: -32211432636416.0000\n",
      "Epoch 32/100, Batch 0/218: D Loss: -32549009096704.0000, GP: 1609731932160.0000, G Adv Loss: -32222446878720.0000, Consistency Loss: 1.1757\n",
      "Epoch 32/100, Batch 50/218: D Loss: -33665205665792.0000, GP: 1661619535872.0000, G Adv Loss: -33553280663552.0000, Consistency Loss: 1.1641\n",
      "Epoch 32/100, Batch 100/218: D Loss: -34602940891136.0000, GP: 1748517257216.0000, G Adv Loss: -35032097882112.0000, Consistency Loss: 1.1681\n",
      "Epoch 32/100, Batch 150/218: D Loss: -36434056577024.0000, GP: 1765737889792.0000, G Adv Loss: -36212731543552.0000, Consistency Loss: 1.1641\n",
      "Epoch 32/100, Batch 200/218: D Loss: -36424996880384.0000, GP: 1840033300480.0000, G Adv Loss: -37463196172288.0000, Consistency Loss: 1.1667\n",
      "Epoch 32 completed. D Loss: -37042364874752.0000, G Loss: -37644247498752.0000\n",
      "Epoch 33/100, Batch 0/218: D Loss: -36966515081216.0000, GP: 1878718808064.0000, G Adv Loss: -37777072717824.0000, Consistency Loss: 1.1670\n",
      "Epoch 33/100, Batch 50/218: D Loss: -39365937987584.0000, GP: 1918284857344.0000, G Adv Loss: -38863338733568.0000, Consistency Loss: 1.1616\n",
      "Epoch 33/100, Batch 100/218: D Loss: -39633987567616.0000, GP: 1958555942912.0000, G Adv Loss: -39203672948736.0000, Consistency Loss: 1.1682\n",
      "Epoch 33/100, Batch 150/218: D Loss: -41751515496448.0000, GP: 2042764722176.0000, G Adv Loss: -40229054447616.0000, Consistency Loss: 1.1659\n",
      "Epoch 33/100, Batch 200/218: D Loss: -43209828859904.0000, GP: 2089347186688.0000, G Adv Loss: -42994619121664.0000, Consistency Loss: 1.1662\n",
      "Epoch 33 completed. D Loss: -42900884815872.0000, G Loss: -42853577261056.0000\n",
      "Epoch 34/100, Batch 0/218: D Loss: -42512496459776.0000, GP: 2151005290496.0000, G Adv Loss: -43353462800384.0000, Consistency Loss: 1.1681\n",
      "Epoch 34/100, Batch 50/218: D Loss: -44956269936640.0000, GP: 2207724339200.0000, G Adv Loss: -45389302464512.0000, Consistency Loss: 1.1642\n",
      "Epoch 34/100, Batch 100/218: D Loss: -46405091590144.0000, GP: 2249349660672.0000, G Adv Loss: -45772875759616.0000, Consistency Loss: 1.1694\n",
      "Epoch 34/100, Batch 150/218: D Loss: -48025330253824.0000, GP: 2340788633600.0000, G Adv Loss: -47862444457984.0000, Consistency Loss: 1.1800\n",
      "Epoch 34/100, Batch 200/218: D Loss: -48802354429952.0000, GP: 2412670615552.0000, G Adv Loss: -48684414795776.0000, Consistency Loss: 1.1705\n",
      "Epoch 34 completed. D Loss: -48956214083584.0000, G Loss: -49047129817088.0000\n",
      "Epoch 35/100, Batch 0/218: D Loss: -49223466745856.0000, GP: 2461953163264.0000, G Adv Loss: -48554890493952.0000, Consistency Loss: 1.1672\n",
      "Epoch 35/100, Batch 50/218: D Loss: -51520380862464.0000, GP: 2535095009280.0000, G Adv Loss: -49508478091264.0000, Consistency Loss: 1.1621\n",
      "Epoch 35/100, Batch 100/218: D Loss: -52174994276352.0000, GP: 2614492921856.0000, G Adv Loss: -52676549148672.0000, Consistency Loss: 1.1682\n",
      "Epoch 35/100, Batch 150/218: D Loss: -54020454481920.0000, GP: 2666007101440.0000, G Adv Loss: -55194079461376.0000, Consistency Loss: 1.1652\n",
      "Epoch 35/100, Batch 200/218: D Loss: -55509616951296.0000, GP: 2762288922624.0000, G Adv Loss: -56467382075392.0000, Consistency Loss: 1.1674\n",
      "Epoch 35 completed. D Loss: -56021338816512.0000, G Loss: -55216305078272.0000\n",
      "Epoch 36/100, Batch 0/218: D Loss: -56016565698560.0000, GP: 2768395304960.0000, G Adv Loss: -56054121496576.0000, Consistency Loss: 1.1663\n",
      "Epoch 36/100, Batch 50/218: D Loss: -57676713164800.0000, GP: 2881032814592.0000, G Adv Loss: -56627893895168.0000, Consistency Loss: 1.1690\n",
      "Epoch 36/100, Batch 100/218: D Loss: -60458958585856.0000, GP: 2956506693632.0000, G Adv Loss: -59902605131776.0000, Consistency Loss: 1.1713\n",
      "Epoch 36/100, Batch 150/218: D Loss: -60325248368640.0000, GP: 3004813541376.0000, G Adv Loss: -60525660602368.0000, Consistency Loss: 1.1576\n",
      "Epoch 36/100, Batch 200/218: D Loss: -62165771878400.0000, GP: 3135297290240.0000, G Adv Loss: -62115914186752.0000, Consistency Loss: 1.1612\n",
      "Epoch 36 completed. D Loss: -64023928242176.0000, G Loss: -63510058893312.0000\n",
      "Epoch 37/100, Batch 0/218: D Loss: -63859335364608.0000, GP: 3157846917120.0000, G Adv Loss: -63601515692032.0000, Consistency Loss: 1.1682\n",
      "Epoch 37/100, Batch 50/218: D Loss: -64239700017152.0000, GP: 3245829783552.0000, G Adv Loss: -64353072054272.0000, Consistency Loss: 1.1664\n",
      "Epoch 37/100, Batch 100/218: D Loss: -66294653124608.0000, GP: 3324938027008.0000, G Adv Loss: -66538103111680.0000, Consistency Loss: 1.1604\n",
      "Epoch 37/100, Batch 150/218: D Loss: -69216090718208.0000, GP: 3453705256960.0000, G Adv Loss: -70021615190016.0000, Consistency Loss: 1.1650\n",
      "Epoch 37/100, Batch 200/218: D Loss: -70449157373952.0000, GP: 3520654737408.0000, G Adv Loss: -71817825878016.0000, Consistency Loss: 1.1725\n",
      "Epoch 37 completed. D Loss: -71309023248384.0000, G Loss: -70848035684352.0000\n",
      "Epoch 38/100, Batch 0/218: D Loss: -70638421147648.0000, GP: 3517441900544.0000, G Adv Loss: -71958704160768.0000, Consistency Loss: 1.1770\n",
      "Epoch 38/100, Batch 50/218: D Loss: -73876700659712.0000, GP: 3668019511296.0000, G Adv Loss: -73653068759040.0000, Consistency Loss: 1.1615\n",
      "Epoch 38/100, Batch 100/218: D Loss: -77441750007808.0000, GP: 3768575328256.0000, G Adv Loss: -76653136969728.0000, Consistency Loss: 1.1695\n",
      "Epoch 38/100, Batch 150/218: D Loss: -77706293149696.0000, GP: 3865195315200.0000, G Adv Loss: -76219915698176.0000, Consistency Loss: 1.1633\n",
      "Epoch 38/100, Batch 200/218: D Loss: -80360499052544.0000, GP: 3999844270080.0000, G Adv Loss: -79716522393600.0000, Consistency Loss: 1.1614\n",
      "Epoch 38 completed. D Loss: -79035610693632.0000, G Loss: -79124571881472.0000\n",
      "Epoch 39/100, Batch 0/218: D Loss: -80434973114368.0000, GP: 3974438846464.0000, G Adv Loss: -80784543186944.0000, Consistency Loss: 1.1652\n",
      "Epoch 39/100, Batch 50/218: D Loss: -86119848869888.0000, GP: 4122691764224.0000, G Adv Loss: -83120183312384.0000, Consistency Loss: 1.1676\n",
      "Epoch 39/100, Batch 100/218: D Loss: -85270217097216.0000, GP: 4232415805440.0000, G Adv Loss: -84122848460800.0000, Consistency Loss: 1.1677\n",
      "Epoch 39/100, Batch 150/218: D Loss: -86284735348736.0000, GP: 4346931576832.0000, G Adv Loss: -88361477865472.0000, Consistency Loss: 1.1602\n",
      "Epoch 39/100, Batch 200/218: D Loss: -89582448148480.0000, GP: 4454607749120.0000, G Adv Loss: -89146274086912.0000, Consistency Loss: 1.1607\n",
      "Epoch 39 completed. D Loss: -90634195042304.0000, G Loss: -89462700769280.0000\n",
      "Epoch 40/100, Batch 0/218: D Loss: -90631158366208.0000, GP: 4465696440320.0000, G Adv Loss: -88472970854400.0000, Consistency Loss: 1.1630\n",
      "Epoch 40/100, Batch 50/218: D Loss: -93272663916544.0000, GP: 4625637310464.0000, G Adv Loss: -92670982619136.0000, Consistency Loss: 1.1682\n",
      "Epoch 40/100, Batch 100/218: D Loss: -93085899948032.0000, GP: 4726074114048.0000, G Adv Loss: -94226725470208.0000, Consistency Loss: 1.1698\n",
      "Epoch 40/100, Batch 150/218: D Loss: -97520822779904.0000, GP: 4846874787840.0000, G Adv Loss: -98714630750208.0000, Consistency Loss: 1.1744\n",
      "Epoch 40/100, Batch 200/218: D Loss: -99566284177408.0000, GP: 5003896946688.0000, G Adv Loss: -97751123623936.0000, Consistency Loss: 1.1646\n",
      "Epoch 40 completed. D Loss: -99808329072640.0000, G Loss: -100109647872000.0000\n",
      "Epoch 41/100, Batch 0/218: D Loss: -98961255825408.0000, GP: 5042910789632.0000, G Adv Loss: -101865668739072.0000, Consistency Loss: 1.1735\n",
      "Epoch 41/100, Batch 50/218: D Loss: -102680236130304.0000, GP: 5193388785664.0000, G Adv Loss: -104171739021312.0000, Consistency Loss: 1.1664\n",
      "Epoch 41/100, Batch 100/218: D Loss: -106298628636672.0000, GP: 5229363331072.0000, G Adv Loss: -105634133442560.0000, Consistency Loss: 1.1615\n",
      "Epoch 41/100, Batch 150/218: D Loss: -108899046785024.0000, GP: 5459354320896.0000, G Adv Loss: -110354327666688.0000, Consistency Loss: 1.1706\n",
      "Epoch 41/100, Batch 200/218: D Loss: -112194595323904.0000, GP: 5554756911104.0000, G Adv Loss: -108199822753792.0000, Consistency Loss: 1.1626\n",
      "Epoch 41 completed. D Loss: -112098436710400.0000, G Loss: -113421580238848.0000\n",
      "Epoch 42/100, Batch 0/218: D Loss: -111042403237888.0000, GP: 5557363671040.0000, G Adv Loss: -113235957121024.0000, Consistency Loss: 1.1700\n",
      "Epoch 42/100, Batch 50/218: D Loss: -115845552406528.0000, GP: 5744089366528.0000, G Adv Loss: -116252047572992.0000, Consistency Loss: 1.1671\n",
      "Epoch 42/100, Batch 100/218: D Loss: -117456324526080.0000, GP: 5848798593024.0000, G Adv Loss: -117853164404736.0000, Consistency Loss: 1.1650\n",
      "Epoch 42/100, Batch 150/218: D Loss: -116892299689984.0000, GP: 6038644326400.0000, G Adv Loss: -120098492776448.0000, Consistency Loss: 1.1662\n",
      "Epoch 42/100, Batch 200/218: D Loss: -123560018313216.0000, GP: 6151045382144.0000, G Adv Loss: -122746499497984.0000, Consistency Loss: 1.1564\n",
      "Epoch 42 completed. D Loss: -125858245246976.0000, G Loss: -123043322003456.0000\n",
      "Epoch 43/100, Batch 0/218: D Loss: -124460510216192.0000, GP: 6251253071872.0000, G Adv Loss: -123942236848128.0000, Consistency Loss: 1.1670\n",
      "Epoch 43/100, Batch 50/218: D Loss: -128829892853760.0000, GP: 6453620375552.0000, G Adv Loss: -128727669276672.0000, Consistency Loss: 1.1665\n",
      "Epoch 43/100, Batch 100/218: D Loss: -131699518210048.0000, GP: 6535093682176.0000, G Adv Loss: -128615815577600.0000, Consistency Loss: 1.1684\n",
      "Epoch 43/100, Batch 150/218: D Loss: -132581513232384.0000, GP: 6655857655808.0000, G Adv Loss: -137126377160704.0000, Consistency Loss: 1.1638\n",
      "Epoch 43/100, Batch 200/218: D Loss: -138895266152448.0000, GP: 6868330086400.0000, G Adv Loss: -137908296089600.0000, Consistency Loss: 1.1593\n",
      "Epoch 43 completed. D Loss: -140048599089152.0000, G Loss: -140768358432768.0000\n",
      "Epoch 44/100, Batch 0/218: D Loss: -137724291973120.0000, GP: 6946991112192.0000, G Adv Loss: -140509712482304.0000, Consistency Loss: 1.1605\n",
      "Epoch 44/100, Batch 50/218: D Loss: -144012803571712.0000, GP: 7043874291712.0000, G Adv Loss: -140631926112256.0000, Consistency Loss: 1.1586\n",
      "Epoch 44/100, Batch 100/218: D Loss: -147571267862528.0000, GP: 7312354836480.0000, G Adv Loss: -144234095050752.0000, Consistency Loss: 1.1642\n"
     ]
    }
   ],
   "source": [
    "# --- 3. CBIGAN Training ---\n",
    "\n",
    "input_dim = X_train_normal_np.shape[1]\n",
    "latent_dim = 128 # Consistent with GANomaly for now\n",
    "\n",
    "# Instantiate models and move to device\n",
    "encoder_model = Encoder(input_dim, latent_dim).to(device)\n",
    "decoder_model = Decoder(latent_dim, input_dim).to(device)\n",
    "discriminator_model = Discriminator(input_dim, latent_dim).to(device)\n",
    "\n",
    "# Optimizers\n",
    "lr_d = 0.0002\n",
    "lr_g = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999 # Adam betas\n",
    "optimizer_d = optim.Adam(discriminator_model.parameters(), lr=lr_d, betas=(beta1, beta2))\n",
    "optimizer_g = optim.Adam(list(encoder_model.parameters()) + list(decoder_model.parameters()), lr=lr_g, betas=(beta1, beta2))\n",
    "\n",
    "# Loss weights for Generator\n",
    "lambda_recon = 10.0 # Weight for reconstruction/consistency loss (often lower than GANomaly)\n",
    "lambda_gp = 10.0 # Weight for gradient penalty\n",
    "\n",
    "EPOCHS = 100 # Number of training epochs\n",
    "CRITIC_ITERS = 5 # Number of discriminator updates per generator update\n",
    "\n",
    "print(\"\\nStarting CBIGAN training...\")\n",
    "history_cbigan = {'d_loss': [], 'g_loss': [], 'gp_loss': [], 'consistency_loss': []}\n",
    "\n",
    "# For gradient penalty\n",
    "def calculate_gradient_penalty(discriminator, real_samples, fake_samples, real_latent, fake_latent):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN-GP.\"\"\"\n",
    "    alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "    \n",
    "    # Expand alpha specifically for data samples (x)\n",
    "    alpha_x = alpha.expand_as(real_samples)\n",
    "    interpolated_x = (alpha_x * real_samples + ((1 - alpha_x) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    # Expand alpha specifically for latent vectors (z)\n",
    "    alpha_z = alpha.expand_as(real_latent)\n",
    "    interpolated_z = (alpha_z * real_latent + ((1 - alpha_z) * fake_latent)).requires_grad_(True)\n",
    "\n",
    "    interpolated_score = discriminator(interpolated_x, interpolated_z)\n",
    "    \n",
    "    # Calculate gradients of D's output w.r.t. interpolated samples (x and z)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=interpolated_score,\n",
    "        inputs=(interpolated_x, interpolated_z),\n",
    "        grad_outputs=torch.ones_like(interpolated_score),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )\n",
    "    # The gradients are returned as a tuple (grad_x, grad_z)\n",
    "    # We need to flatten both and concatenate them for the norm calculation\n",
    "    grad_x_flattened = gradients[0].view(gradients[0].size(0), -1)\n",
    "    grad_z_flattened = gradients[1].view(gradients[1].size(0), -1)\n",
    "    \n",
    "    # Concatenate gradients of x and z to compute the norm of the combined gradient\n",
    "    combined_gradients = torch.cat([grad_x_flattened, grad_z_flattened], dim=1)\n",
    "\n",
    "    gradient_penalty = ((combined_gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (real_data_batch,) in enumerate(train_loader):\n",
    "        real_data = real_data_batch.view(real_data_batch.size(0), -1).to(device)\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # --- Train Discriminator ---\n",
    "        for _ in range(CRITIC_ITERS):\n",
    "            optimizer_d.zero_grad()\n",
    "\n",
    "            # 1. Real pair (x, E(x))\n",
    "            z_encoded_real = encoder_model(real_data).detach() # Detach E(x)\n",
    "            d_real = discriminator_model(real_data, z_encoded_real)\n",
    "\n",
    "            # 2. Fake pair (G(z), z)\n",
    "            z_random = torch.randn(batch_size, latent_dim).to(device) # Random latent vector\n",
    "            fake_data_from_z = decoder_model(z_random).detach() # Detach G(z)\n",
    "            d_fake = discriminator_model(fake_data_from_z, z_random)\n",
    "\n",
    "            # WGAN Loss for D: D(fake) - D(real)\n",
    "            d_loss = d_fake.mean() - d_real.mean()\n",
    "\n",
    "            # Gradient Penalty\n",
    "            gp = calculate_gradient_penalty(discriminator_model, real_data, fake_data_from_z, z_encoded_real, z_random)\n",
    "            d_loss_total = d_loss + lambda_gp * gp\n",
    "            d_loss_total.backward()\n",
    "            optimizer_d.step()\n",
    "        \n",
    "        # --- Train Generator (Encoder and Decoder) ---\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "\n",
    "        # Term 1: Maximize D's output for (x, E(x))\n",
    "        # Feed real_data through encoder\n",
    "        z_encoded_real_g = encoder_model(real_data)\n",
    "        d_real_g = discriminator_model(real_data, z_encoded_real_g)\n",
    "\n",
    "        # Term 2: Minimize D's output for (G(z), z)\n",
    "        # Generate fake data from random latent\n",
    "        z_random_g = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_data_from_z_g = decoder_model(z_random_g)\n",
    "        d_fake_g = discriminator_model(fake_data_from_z_g, z_random_g)\n",
    "\n",
    "        # Adversarial loss for Generator\n",
    "        # G wants d_real_g to be high and d_fake_g to be low (making the generator components look real to D)\n",
    "        # So, the loss is -(d_real_g.mean() - d_fake_g.mean())\n",
    "        g_loss_adv = -d_real_g.mean() + d_fake_g.mean()\n",
    "\n",
    "        # Consistency Loss: E(D(z)) should be close to z\n",
    "        # Take the generated data from z_random_g and encode it\n",
    "        z_reconstructed_from_fake_x = encoder_model(fake_data_from_z_g)\n",
    "        consistency_loss = (z_reconstructed_from_fake_x - z_random_g).abs().mean() # L1 loss for consistency\n",
    "\n",
    "        g_loss_total = g_loss_adv + lambda_recon * consistency_loss\n",
    "        g_loss_total.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        if i % 50 == 0: # Print more frequently to see training progress\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}, Batch {i}/{len(train_loader)}: \"\n",
    "                  f\"D Loss: {d_loss.item():.4f}, GP: {gp.item():.4f}, \"\n",
    "                  f\"G Adv Loss: {g_loss_adv.item():.4f}, Consistency Loss: {consistency_loss.item():.4f}\")\n",
    "\n",
    "    # Store metrics for plotting\n",
    "    history_cbigan['d_loss'].append(d_loss.item())\n",
    "    history_cbigan['gp_loss'].append(gp.item())\n",
    "    history_cbigan['g_loss'].append(g_loss_adv.item()) # Store adversarial part of G loss\n",
    "    history_cbigan['consistency_loss'].append(consistency_loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed. D Loss: {history_cbigan['d_loss'][-1]:.4f}, G Loss: {history_cbigan['g_loss'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\nCBIGAN training finished.\")\n",
    "\n",
    "\n",
    "# --- 4. Anomaly Detection and Evaluation for CBIGAN ---\n",
    "\n",
    "def cbigan_anomaly_score(encoder, decoder, discriminator, data_tensor, latent_dim=128):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    discriminator.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encode the input data x -> z_x\n",
    "        z_x = encoder(data_tensor)\n",
    "\n",
    "        # Reconstruct x from z_x -> x_recon\n",
    "        x_recon = decoder(z_x)\n",
    "\n",
    "        # Rconsistency check\n",
    "        z_x_recon = encoder(x_recon)\n",
    "\n",
    "        # MAE\n",
    "        recon_error = torch.mean(torch.abs(data_tensor - x_recon), dim=1)\n",
    "\n",
    "        # MAE between z_x and z_x_recon\n",
    "        latent_consistency_error = torch.mean(torch.abs(z_x - z_x_recon), dim=1)\n",
    "\n",
    "        # 3. Discriminator Score for (x, E(x)) pair\n",
    "        # Higher D(x, E(x)) means more \"normal\" to D.\n",
    "        # So for anomaly score, we want (1 - D_score) if D outputs [0,1] or just -D_score if linear.\n",
    "        # Since WGAN-GP D output is linear, we use -D_score, meaning lower D score implies more anomalous.\n",
    "        d_score = discriminator(data_tensor, z_x).squeeze()\n",
    "    \n",
    "        anomaly_score_val = -d_score # Negative discriminator score, higher means more anomalous\n",
    "\n",
    "    return anomaly_score_val.cpu().numpy()\n",
    "\n",
    "\n",
    "print(\"\\nCalculating anomaly scores for test data using CBIGAN...\")\n",
    "anomaly_scores_cbigan = cbigan_anomaly_score(encoder_model, decoder_model, discriminator_model, X_test_full_tensor)\n",
    "\n",
    "# Determine a threshold for anomaly detection\n",
    "print(\"\\nCalculating anomaly scores for normal training data to determine threshold for CBIGAN...\")\n",
    "normal_train_anomaly_scores_cbigan = cbigan_anomaly_score(encoder_model, decoder_model, discriminator_model, X_train_normal_tensor)\n",
    "\n",
    "threshold_cbigan = np.percentile(normal_train_anomaly_scores_cbigan, 95)\n",
    "print(f\"Calculated anomaly threshold (95th percentile of normal training scores for CBIGAN): {threshold_cbigan:.4f}\")\n",
    "\n",
    "# Predict anomalies based on the threshold\n",
    "y_pred_anomaly_cbigan = (anomaly_scores_cbigan > threshold_cbigan).astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a2737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save Trained CBIGAN Models ---\n",
    "torch.save(encoder_model.state_dict(), 'cbigan_encoder.pth')\n",
    "torch.save(decoder_model.state_dict(), 'cbigan_decoder.pth')\n",
    "torch.save(discriminator_model.state_dict(), 'cbigan_discriminator.pth')\n",
    "\n",
    "print(\"CBIGAN models saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77cd0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure y_test_labels is numpy array\n",
    "y_test_np = y_test_labels.cpu().numpy()\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test_np, y_pred_anomaly_cbigan)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n",
    "            xticklabels=['Normal', 'Attack'], yticklabels=['Normal', 'Attack'])\n",
    "plt.title('CBIGAN Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# --- ROC & PR Curves ---\n",
    "fpr_cbigan, tpr_cbigan, _ = roc_curve(y_test_np, anomaly_scores_cbigan)\n",
    "roc_auc_cbigan = auc(fpr_cbigan, tpr_cbigan)\n",
    "\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test_np, anomaly_scores_cbigan)\n",
    "pr_auc_cbigan = auc(recall_vals, precision_vals)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr_cbigan, tpr_cbigan, color='darkorange', lw=2, label=f'AUC = {roc_auc_cbigan:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.title('CBIGAN ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# PR Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall_vals, precision_vals, color='blue', lw=2, label=f'AUC = {pr_auc_cbigan:.2f}')\n",
    "plt.title('CBIGAN Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Additional Metrics ---\n",
    "y_pred = y_pred_anomaly_cbigan\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_np, y_pred, average='binary', zero_division=0)\n",
    "accuracy = accuracy_score(y_test_np, y_pred)\n",
    "\n",
    "print(f\"\\n--- CBIGAN Evaluation Metrics ---\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_cbigan:.4f}\")\n",
    "print(f\"PR AUC:    {pr_auc_cbigan:.4f}\")\n",
    "print(f\"Threshold: {threshold_cbigan:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "print(\"\\n--- CBIGAN Performance Evaluation ---\")\n",
    "auc_roc_cbigan = roc_auc_score(y_test_full, anomaly_scores_cbigan) # AUC-ROC with raw anomaly scores\n",
    "print(f\"AUC-ROC Score: {auc_roc_cbigan:.4f}\")\n",
    "\n",
    "precision_cbigan, recall_cbigan, f1_cbigan, _ = precision_recall_fscore_support(y_test_full, y_pred_anomaly_cbigan, average='binary', zero_division=0)\n",
    "accuracy_cbigan = accuracy_score(y_test_full, y_pred_anomaly_cbigan)\n",
    "conf_matrix_cbigan = confusion_matrix(y_test_full, y_pred_anomaly_cbigan)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_cbigan:.4f}\")\n",
    "print(f\"Precision: {precision_cbigan:.4f}\")\n",
    "print(f\"Recall: {recall_cbigan:.4f}\")\n",
    "print(f\"F1-Score: {f1_cbigan:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix_cbigan)\n",
    "\n",
    "# Plotting training losses for CBIGAN\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_cbigan['d_loss'], label='Discriminator Loss (WGAN)')\n",
    "plt.plot(history_cbigan['g_loss'], label='Generator Adversarial Loss')\n",
    "plt.plot(history_cbigan['gp_loss'], label='Gradient Penalty Loss')\n",
    "plt.plot(history_cbigan['consistency_loss'], label='Consistency Loss')\n",
    "plt.title('CBIGAN Training Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting anomaly score distributions for CBIGAN\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(normal_train_anomaly_scores_cbigan, color='blue', label='Normal Training Data Anomaly Scores', kde=True, stat='density', alpha=0.6)\n",
    "sns.histplot(anomaly_scores_cbigan[y_test_full == 0], color='green', label='Normal Test Data Anomaly Scores', kde=True, stat='density', alpha=0.6)\n",
    "sns.histplot(anomaly_scores_cbigan[y_test_full == 1], color='red', label='Attack Test Data Anomaly Scores', kde=True, stat='density', alpha=0.6)\n",
    "plt.axvline(threshold_cbigan, color='purple', linestyle='--', label=f'Threshold ({threshold_cbigan:.4f})')\n",
    "plt.title('Distribution of Anomaly Scores (CBIGAN)')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
